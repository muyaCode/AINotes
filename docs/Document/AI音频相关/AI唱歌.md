# AI唱歌

## 介绍

## 软件

- so-vits-svc github地址：<https://github.com/svc-develop-team/so-vits-svc>
- sadtalker github地址：<https://github.com/Winfredy/SadTalker>

视频留言里【【AI唱歌】再次进化！6分钟学会用AI唱歌，杀疯了！】<https://www.bilibili.com/video/BV1wv4y1J7pR?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

一键启动包下载地址（提取码：qi2p）： <https://pan.baidu.com/s/1Jm-p_DZ2IVcNkkOYVULerg?pwd=qi2p>

## AudioCraft整合包

官方的说明：We recommend 16GB of memory, but smaller GPUs will be able to generate short sequences, or longer sequences with the small model.最好是16G往上的，低的显存只能生成短的音频，并且只能用small这个模型

唯一需要安装的是基础环境python3.10的版本，可通过这个连接下载：
<https://www.python.org/ftp/python/3.10.9/python-3.10.9-amd64.exe>

整合包下载地址：链接：<https://pan.baidu.com/s/1M0c-6_6yI2IMBjo0LN7HNg?pwd=gfrj>   提取码：gfrj

代码地址：<https://github.com/facebookresearch/audiocraft>

Colab地址：<https://github.com/camenduru/MusicGen-colab>

在线试玩：<https://huggingface.co/spaces/facebook/MusicGen>

## 如何训练一个AI歌手

视频教程：【手把手教学！如何自己训练一个AI歌手 - sovits本地&云端训练教程】<https://www.bilibili.com/video/BV1ea4y1G7gx?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

- sovits：<https://github.com/svc-develop-team/so-vits-svc>
- 一键包：<https://www.bilibili.com/video/BV1Cc411H74D/>
- UVR5：<https://www.bilibili.com/video/BV1ga411S7gP/>
- RX Audio Editor
- 123盘：<https://www.123pan.com/s/RiyA-LjS03>
- 夸克网盘：<https://pan.quark.cn/s/f9791f6790d3>
- 百度网盘：<https://pan.baidu.com/s/1xUXd9vVHR11sjJ6wCVuwHQ?pwd=hjhj> 提取码: hjhj
- Audio Slicer：
- Github链接：<https://github.com/flutydeer/audio-slicer/blob/main/README.zh-CN.md>
- 网盘链接：<https://henji.lanzout.com/iuSOk0uv354j>

————————————————————

AI声源：

- 孙燕姿

相关歌曲：

- 梁静茹 - 宁夏
- RADWIMPS&十明 - すずめ feat.十明
- 林俊杰 - 可惜没如果
- 米津玄师 - Lemon

【AI歌手+AI变声二合一，还能一键制作音乐视频！最新RVC模型训练教程，在线一键训练，效果提升明显，快来试试吧！】<https://www.bilibili.com/video/BV1r8411f7MR?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

程序连接：<https://huggingface.co/spaces/kevinwang676/Voice-Changer>

详细训练教程：【【RVC】在线一键训练AI歌手，无需配置环境，只需半小时！还能生成音乐视频！使用Autodl镜像快速训练，快来试试吧！】<https://www.bilibili.com/video/BV1mX4y1C7w4?vd_source=36c9491a7fa2ab8a22ca060af01b7472；>

在线训练链接：<https://www.autodl.com/home；>

——————————————————————————————————————————————————————————————————————————————————————————————————————————————————

[**⋙ 完整教程@数字生命卡兹克**](https://mp.weixin.qq.com/s/bBqGwsDgStTGTQs_ha03xg)

**借助 So-VITS-SVC，用自己的声音完整唱了一首「富士山下」**。作者用大量的图文，详细写了整个AI声音教程，并准确了完整安装包。

> *1*. **准备声音数据集**。声音模型对数据集的要求比较苛刻，想训练自己的声音需要录制1小时以上的无杂音的纯人声，WAV 格式，再使用 Audio Slicer (音频切分工具) 将其剪裁成10秒左右的分段文件
>
> *2*. **租云算力，上传数据集**。训练模型挺烧显卡的，直接找到便宜稳定的云算力平台，充值三五十元就可以搞定！轻松便捷
>
> *3*. **在云上训练模型**。跟着截图在云平台的控制台操作，进行 10,000 步的模型训练，耗时较久，做好准备
>
> *4*. **本地进行推理模型重绘歌曲**。声音重绘的原理是，用模型的音色替换人声。这一步推荐了大量实用工具，比如分离工具「UVR5」、音乐获取「QQ音乐」、歌曲合成「AU/剪映」等
>
> 注意！项目完成时云算力还在烧钱！如果不继续使用，直接先点关机，然后点击释放实例

## 音乐生成与创作AI

【音乐生成与创作AI，Meta开源Audiocraft 安装与使用教程】<https://www.bilibili.com/video/BV1bh4y1X77a?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

Meta开源音乐生成大模型audiocraft，可根据文本提示和音乐样本创作新的乐曲，生成效果非常逼真，媲美艺术家作品：<https://github.com/facebookresearch/audiocraft>

安装说明文档在百度网盘：链接：<https://pan.baidu.com/s/17O8W112laAeSxz3pT0qa5w>   提取码：ro7s

## Meta 音乐生成模型 MusicGen，通过文字/音频来生成或修改音乐

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/87883384905c4756ab3a5347a4ce994e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

Meta近日在HuggingFace开源了一款新的音乐生成模型MusicGen，可以将文本和已有旋律转化为完整乐曲，也支持对现有音乐进行修改。

研发团队表示：我们使用了20000小时的授权音乐来训练该模型，并采用Meta的EnCodec编码器将音频数据分解为更小的单元进行并行处理，进而让MusicGen的运算效率和生成速度都比同类型AI模型更为出色 [MusicGen - a Hugging Face Space by facebook](https://huggingface.co/spaces/facebook/MusicGen)

## 教你打造属于自己的AI孙燕姿』AI歌手模型使用及训练保姆级课程

最近AI歌手很火啊！歸藏分享了他使用 So-VITS-SVC 项目包来推理和训练的详细全过程，并分享了所有的资料包和使用教程。

> 最终「AI孙燕姿」唱了一首「爱得太迟」，还挺不错！

### 🔔 第一部分：使用模型

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73099c611cdf4fbda71ed85ab8d7abdb~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

如果只想使用别人训练好的模型尝鲜，那么只看这部分就可以了。

> *1*. **原始声音处理**。准备一段高品质、清晰的演唱音频，并去掉背景音乐提取人声
>
> *2*. 推理过程。启动 webui.bat 并使用推理功能，选择模型与配置文件，加载模型后勾选「聚类f0」「F0均值滤波」两个选项，并通过音频转换生成音乐
>
> *3*. 音轨合并。下载当前生成的干声音频，并于第1步剥离的伴奏音频进行合成，就可以生成带有一首非常不错的歌曲啦！当然加上图片就可以生成视频~ [**⋙ 阅读全文**](https://mp.weixin.qq.com/s/bXD1u6ysYkTEamt-PYI1RA)

### 🔔 第二部分：训练模型

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/72b0f63174504722811f1056d005d431~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

> *1*. **数据准备**。准备质量比较高、人声比较清晰声音素材，并对素材进行格式转换、半轴分离、文件分割等处理
>
> *2*. **模型训练**。识别数据集、数据预处理、设置参数、开始训练，并获得模型保存的结果
>
> *3*. 音轨合并。回到上方第3步的同样操作 [**⋙ 阅读全文**](https://mp.weixin.qq.com/s/IeeW1PbMUbxMlLl529JTYQ)

## RVC 模型

使用 RVC 模型和入梦工具，带大家实现以下几个功能：

- 音乐干声分离：背景音（BGM）与人声（干声）的分离
- 训练个人音色模型：作为模仿其他干声素材的音色数据
- 男女换声（伪音）：基于异性干声素材，进行实时转化声音为异性声音
- AI 唱歌：仅作基础的模拟演唱，仍需进行调音等等操作，才可以达到完美
- 音色融合：不同音色的特征融合出一个全新的音色

在进行教学前，我们先假设这样一个场景：现有素材，男声音色A，女声音色B，女声音色C的朗读素材C，男唱歌状态下的音色D，女声音色C的唱歌素材E，我们根据以上介绍的功能进行整合，可以做到以下案例：

- 案例一：将女声C的朗读素材进行干声分离，再用男声音色A朗读女声音色C的朗读素材C。
- 案例二：可以用女声B的音色去朗读女声C的朗读素材。
- 案例三：甚至能够进行小延迟（0.1s）的实时音色转换，比如将使用男声音色A的声音去讲，可以实时转化为女声音色B的声音，实现无技巧完成男女伪声转换。
- 案例四：利用音色D的声音去演唱音色C的唱歌素材，实现类似AI孙燕姿的功能。
- 案例五：拿女音色B和C的素材进行融合出一个新的音色F
- 案例六：音色A从来没有说过外语（英语、日语等等），但需要现在马上说一段外语音频

效果可参考出处：[【rvc教程】AI变声/AI音色训练_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ek4y1W7Xv/?vd_source=5f0c99b3deddffe219938763769b15ac)  极为逼真，值得尝试。

下载资源： 文件目录如下：

![文件目录](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d299df65000a4896ad6f30b743a88c0a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

### 注意事项

- 性能要求：建议20系N卡以上，显存在 8g 以上，性能越高越好
- 模型文件目录要求：全英文、无中文、待处理音频、单音色素材独立一个文件夹
- 启动模型时，命令行界面不能关闭，否则，模型停止运行，下文中提到的所有命令行都使用时不能关闭
- 注意音色版权问题，不做违法勾当，技术无罪，请勿滥用
- 音色、音频素材质量好坏不仅仅体现在音源质量，音色训练干声素材更在意有无噪声（气泡音、混响者等不佳），是否贴合模仿场景（唱歌音色对应唱歌音频转换等等），是否音频变调范围较少等等

### 音色推理流程

什么是音色推理呢？其实就是实现案例一二的过程，也就是推理音色A到音色B，再应用到声音素材上的过程。我们将使用 RVC 模型的一建训练包，步骤如下：

1. 启动 RVC web 界面：双击打开 RVC-beta_5\RVC-beta\go-web.bat
2. 等待启动，启动成功命令行效果如下： 命令行： ![go-web.bat](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/60a20686f4f646c2800563254517b888~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) web界面： ![web界面](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e76b1e086ba147d588b13aa334b090a5~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
3. 界面参数说明：
   - 推理音色：推理结果成品的实际音色
   - 待处理音频：推理结果成品的实际音频内容，支持绝大部分音频格式
   - index 路径：推理音色相符合的特征文件 index 结尾
   - 变调(整数, 半音数量, 升八度12降八度-12)：男女音调差距较大，男转女推荐+12key, 女转男推荐-12key, 如果音域爆炸导致音色失真也可以自己调整到合适音域.
   - 刷新音色列表和索引路径：加载新的推理音色和 index 文件，训练出新音色就需要重新加载
   - 卸载音色：去除加载进的音色，以节省显存
   - 音高提取算法：输入歌声可用pm提速,harvest低音好但巨慢无比
4. 选定对应参数数据：按照界面参数，选择推理音色、待处理音频、index 路径、变调等等最基本的参数（也就是说其他参数保持默认的参数也能使用，有能力、有需求的再自行微调），点击转换即可进行音色推理了。 转换结果效果如下： ![音色推理结果](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/06f9c241074f4fa1b1722ce8017ceebf~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 点击播放按钮即可在线播放转换后的音频，右键点击即可下载结果音频或者改变播放速度。最好是先听听效果，再下载，毕竟不一定效果合适，可能需要调整参数。 这就是音色推理的全流程，也是整个模型训练性能要求最低的一个部分之一，如果，你连默认的音色都无法正常推理成功的话，音色训练部分建议在性能更强的电脑上进行。

### 素材干声分离

素材干声分离也就是分离人声和背景音，这一部分不一定需要本模型一键训练包来完成，只是为了获得更好的干声素材而做的预处理操作，有其他现成更简单的工具也可以使用。话扯远了，接下来就说说怎么进行素材的干声分离，步骤如下：

1. 启动 RVC 模型的一建训练包，和推理模型的启动方法一致，切换到干声分离界面，如下： ![素材干声分离界面](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/88114ddfe8a34fd6a03ba3bf60fd9f56~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
2. 界面参数说明：
   - 待处理音频路径：待处理音频的文件夹路径，注意不是文件路径，这也是为什么每个待处理音频都要独立放置到一个文件夹的原因，因为太多文件，训练时间过长。
   - 按需选择分离模型：HP2 人声（只有背景音和人声类型）、HP5人声（带有背景音和人声叠加等等效果类型）
   - 指定人声输出目录：默认 RVC-beta\opt
   - 指定乐器文件夹：背景音文件夹，默认 RVC-beta\opt
3. 填写好对应参数信息之后，点击转换即可完成素材干声分离。最终效果如下： ![素材干声分离结果](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6ed6073586c4a959caa3c5a4c3875b6~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 输出信息为 success 即为干声分离成功，如果报错，就需要检查音频和电脑的硬件问题了。

### 音色训练

音色训练其实就是利用经过预处理或者本身音源素质良好的干声素材进行训练，提取对应的音色特征，进而模拟其音色特征，再生成对应的音色包，这个过程中当然可以尝试通过微调参数实现更好的音色特征提取，但篇幅有限，本文只介绍最简单，最直接的音色训练教程。步骤如下：

1. 准备好 3 分钟以上、50 分钟以内的优质干声素材，建议在3到7分钟之间，效果就很不错了，做好训练时长等待的准备
2. 启动训练模型，切换到训练模块，如下： ![训练界面](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/512d4e92a53c4c25bbe67f6ef2765912~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
3. 界面参数说明：
   - 实验名：即将训练出来的音色包名称
   - 目标采样率：干声素材采集样本占比，按性能需求更改，默认 40k 就有不错的效果了
   - 模型是否带音高指导：如果是唱歌类型的干声素材，必须选择 true ，反之，选或不选都可以
   - 版本：建议使用 V1，V2 仍存在部分 Bug
   - 提取音高和处理数据所使用的 CPU 进程数，默认为 16 ，可根据性能瓶颈自行更改，最少为 2
   - 训练文件夹路径：所要训练的干声素材文件夹路径，注意是文件夹路径，同一个文件夹里面只能包含一个人的音色干声素材
   - 显卡信息：启动后会自动读取本机显卡信息，多张显卡可输入卡号，指定训练用显卡
   - 音高提取算法：输入歌声可用pm提速,高质量语音但CPU差可用dio提速,harvest质量更好但慢
   - 保存频率：每训练 n 轮，保存一次音色特征数据，建议以 20 为保存频率，可根据性能瓶颈自行更改
   - 总训练轮数：不得小于保存频率数，总训练轮数按性能瓶颈来，建议 200 轮即可，干声素材优秀可选择 50 轮即可，普通人听不出来的，最高可达 1000 轮，轮数越高，性能要求越大，时间越长，过高也会过拟合，不建议太高。
   - 每张显卡的 back_size：按默认即可，会在读取显卡信息后自动选择，如果自行指定训练显卡，可根据性能瓶颈自行选择
   - 是否仅保存最新的ckpt文件以节省硬盘空间：选择是的话，只有最后一轮的训练特征数据，反之，按保存频率保存音色文件
   - 是否缓存所有训练集至显存.：10min以下小数据可缓存以加速训练, 大数据缓存会炸显存也加不了多少速度
   - 是否在每次保存时间点将最终小模型保存至weights文件夹：选择是即可
4. 填好以上界面参数数据，点击一键训练即可，慢慢等待结果出现，建议只运行该模型 ckpt 处显示 success，结尾有个 2333333 即为成功 ![训练结果命令行](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a9f050e4dfa348d18cfe3ce347e37d71~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 训练结果（音色）文件夹：weights 文件夹 ![音色文件夹](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/869b8b5f25d147aba8d63ff4800d4b1b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 训练音色特征结果：logs 文件夹 ![音色特征数据](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b0e4a47e918477fa955934d0fefcf79~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 如果，没有迁移训练和微调参数需求的话，可仅保存 index 和 npy 文件，连同前文的 pth 文件就可以构成一个完整的音色包文件。可分享音色包文件示例如下： ![音色包文件](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f78bdb1b52504c35ba46a39d3bdd5c87~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

利用这个音色包文件就可以进行前文的音色推理了，也就能够实现案例一、二、四、七，包含唱歌类型的模拟，以及说一段自己不会的外语音频也是可以实现的，同时，实现唱歌素材的模拟不就可以得到 AI 孙燕姿的干声素材，再利用之前干声分离出的背景音进行调整，就可以基本实现AI孙燕姿啦，只要我们拥有（训练所得、分享获得）孙燕姿的唱歌音色包，以及对应优质的唱歌干声素材，当然，还需要进一步的调音、编曲等待操作，让它更像更完美。

### 音色融合

音色融合就是基于前文提到的音色训练出来的音色包进行融合音色，当然，同一性别的音色融合效果会好一点，通过音色融合，我们就可以创造出一个全新的音色包，当然，也可以利用此操作减少音色爆音的几率，比如，A音色音色好但容易爆音，B音色不太好，但胜在稳定，就可以用高权重的A模型融合低权重的B模型，融合出来的音色就能拥有两个的优点，但权重比例得自行调整，达到一个比较好的平衡，基于融合后的音色就可以做出不一样的音源素材。步骤如下：

1. 启动模型，切换到 ckpt 部分 ![ckpt界面](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/781419969ffb4f20bfc80c998970c916~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

2. 界面参数说明：

   - A、B模型路径：A、B模型文件路径就是需要进行融合的两个音色模型的 pth 文件路径
   - A 模型权重：融合哪个音源特征更多的数值化表达，也就是融合结果音色更像哪个音色
   - 保存的模型名称：此次融合结果音色的名称

   其他参数自行调整，简单地调整以上数据就能实现音色融合

3. 点击融合，输出结果为 success 即可。音色包会在 weight 出现，并且，不会有对应的 index 和 npy 文件生成，但可用高权重的音色模型训练出来的结果文件。 ![音色融合](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0841c99a5f547d591ece4f7e6799c1e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 这我们就能实现案例五的效果了，你可以使用融合后的音色继续“炼丹”，直到满意为止。

### 入梦工具实现男女伪音实时无技巧转化

男女伪音，也就是男女声音实时互换的操作，实现类似于变声器的效果，部分游戏、陪玩也有在使用。步骤如下：

1. 启动模型和入梦工具：双击 RVC-beta_5\RVC-beta\go-realtime-gui.bat 和 RVC\RVC入梦小工具\RVC入梦小工具.exe GUI 界面如下： ![模型GUI界面](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b67d81770db84e24a851d3734861f295~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

   模型运行命令行界面如下： ![模型命令行](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6aa6dee92d2844a4b57ff92e9ae10a3f~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) 入梦工具界面如下： ![入梦工具](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/675532e7182c4434a0b54a118f8d35e7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

2. 安装入梦工具驱动：点击入梦工具虚拟 MME，一直下一步即可安装驱动 ![驱动安装](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d1798570c9424cbd948d8b375fb15c02~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

3. 点击系统音频，配置扬声器和麦克风

   - 录制设备配置成入梦工具为默认使用设备 ![配置录制设备](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1caf834185c040769bb906eac5067301~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

   - 播放设备不用修改，使用电脑默认设置就行，需要修改入梦扬声器的属性配置中的采样频率和位深度与电脑默认设置的设备对应属性一致，再更改入梦麦克风的侦听属性为侦听此设备即可。不过建议为耳机类型的扬声器，公放类型会被录制设备读取，产生回音，出现杂音。 ![修改属性一](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/65cb709a869d43d0bda44ec1a7460b17~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) ![默认设备属性](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2bcae4cd268240d28d3f61833bb4ce3a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

     ![修改属性二](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c2349086ed524ac6b15cf15b7b78e796~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp) ![修改属性三](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e29425d861c451585adb9aa141dddc4~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

4. 配置模型音频输入输出设备：输入设备设置为电脑默认麦克风即可，输出设备设置为入梦扬声器，实际播放声音为电脑默认音频输出设备，如下： ![音频输入输出设备设置](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91e544ce9bdc425698327fcf29884b75~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

5. 模型 GUI 界面加载模型参数说明： ![加载模型](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f4d9a3c62b724b0fa1bd3592933d2839~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

   - 载入 Hubert 模型：不会默认读取模型，需要自行载入，双击按钮打开文件夹，选中 RVC-beta_5\RVC-beta\hubert_base.pt 即可。
   - 选择 pth 文件：双击按钮打开文件夹，自行选中音色包内的 pth 文件即可
   - 选择 index 文件：双击按钮打开文件夹，自行选中对应音色包内的 index  文件即可
   - 选择 npy 文件：双击按钮打开文件夹，自行选中对应音色包内的 npy 文件即可

6. 模型 GUI 界面常规设置及其性能设置参数说明： ![常规设置及其性能设置](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d48096eaf1e43e38e094153770dee62~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

   - 响应阈值：麦克风读取响应速度，自行调整不爆音即可，数据越低，实时延迟越小
   - 音调设置：男女音调差距较大，男转女推荐+12key, 女转男推荐-12key, 如果音域爆炸导致音色失真也可以自己调整到合适音域。
   - index rate：0.3 到 0.5即可，特征提取相关参数
   - 采样长度：推理时间，采样长度自行调整，不含过多电子音即可，数值越低，延迟越低，建议为 1 即可
   - 淡入淡入长度：保持默认即可，除非有一些奇怪的尾音
   - 额外推理长度：推理长度高一点，声音可能会好一些，但延迟高，自行调整
   - 输入输出降噪：如果录音环境存在一定噪声可勾选

7. 选择完其他设置，点击开始转化即可实现实时转换音色的效果，注意推理时间正常变化才是正常运行。

8. 如果需要切换音色的话，就必须停止音频转换再重新修改加载模型部分的参数。

9. 如果是游戏使用、录制时使用，就必须把对应软件的麦克风设置为入梦麦克风，注意检查一下

本章节解决了案例三的男女伪音转换效果，这也将引起我们的警觉。

### 总结一下

AI 语音技术的进步已经带来了许多令人兴奋的结果，而 RVC 则是其中的一个重要发展方向。RVC 可以让使用者将一个人的声音样本複制并转移到另一个人身上，并可实现即时语音转换。以下是 RVC 可能带来的一些结果：

1. 更自然的语音转换：RVC 技术可以让语音转换更加自然、逼真。这种技术可以学习一个人的语音特徵，包括音调、节奏和语速等，并将这些特徵应用到其他人的语音中，使其听起来更加自然。
2. 音频和影片后期制作：RVC 技术还可以用于音频和影片后期制作。例如，在电影和电视剧中，演员的声音可能需要进行修剪或处理，RVC 技术可以帮助制作人员快速、高效地完成这些任务。
3. 音乐创作：RVC 技术可以用于音乐创作，例如合成电子音乐或增强现有音乐。使用这种技术，音乐家可以从其他艺术家的声音中获得灵感，并将其应用到自己的创作中。

## 基于人工智能声音克隆框架PaddleSpeech克隆出精致细腻声音

虽然这技术对于娱乐、语音合成等方面有著极大的应用价值。然而，这种技术也引发了许多道德等问题，例如滥用、欺骗、侵犯隐私等问题，需要你我共同关注，使用该技术时也要特别注意这些问题，请小心别踩线。

工智能AI技术可以让我们基于PaddleSpeech克隆出精致细腻的国师原声，普通人也可以玩转搞笑配音。

### 数据集准备和清洗

我们的目的是克隆国师的声音，那么就必须要有国师的声音样本，这里的声音样本和[使用so-vits-svc4.0克隆歌声一样](https://v3u.cn/a_id_310)，需要相对“干净”的素材，所谓干净，即没有背景杂音和空白片段的音频素材，也可以使用国师采访的原视频音轨。

需要注意的是，原视频中女记者的提问音轨需要删除掉，否则会影响模型的推理效果。

随后，将训练集数据进行切分，主要是为了防止爆显存问题，可以手动切为长度在5秒到15秒的音轨切片，也可以使用三方库进行切分：

```bash
git clone https://github.com/openvpi/audio-slicer.git
```

随后编写脚本：

```ini
import librosa  # Optional. Use any library you like to read audio files.  
import soundfile  # Optional. Use any library you like to write audio files.  
  
from slicer2 import Slicer  
  
audio, sr = librosa.load('国师采访.wav', sr=None, mono=False)  # Load an audio file with librosa.  
slicer = Slicer(  
    sr=sr,  
    threshold=-40,  
    min_length=5000,  
    min_interval=300,  
    hop_size=10,  
    max_sil_kept=500  
)  
chunks = slicer.slice(audio)  
for i, chunk in enumerate(chunks):  
    if len(chunk.shape) > 1:  
        chunk = chunk.T  # Swap axes if the audio is stereo.  
    soundfile.write(f'master_voice/{i}.wav', chunk, sr)  # Save sliced audio files with soundfile.
```

注意这里min_length的单位是毫秒。

由于原始视频并未有背景音乐，所以分拆之前我们不用拆分前景音和背景音，如果你的素材有背景音乐，可以考虑使用spleeter来进行分离，具体请参照：[人工智能AI库Spleeter免费人声和背景音乐分离实践(Python3.10)](https://v3u.cn/a_id_305)，这里不再赘述。

如果对原视频的存在的杂音不太满意，可以通过noisereduce库进行降噪处理：

```python
from scipy.io import wavfile  
import noisereduce as nr  
# load data  
rate, data = wavfile.read("1.wav")  
# perform noise reduction  
reduced_noise = nr.reduce_noise(y=data, sr=rate)  
wavfile.write("1_reduced_noise.wav", rate, reduced_noise)
```

训练集数量最好不要低于20个，虽然音频训练更适合小样本，但数量不够也会影响模型质量。

最后我们就得到了一组数据集：

```yaml
D:\work\speech\master_voice>dir  
 驱动器 D 中的卷是 新加卷  
 卷的序列号是 9824-5798  
  
 D:\work\speech\master_voice 的目录  
  
2023/06/13  17:05    <DIR>          .  
2023/06/13  20:42    <DIR>          ..  
2023/06/13  16:42           909,880 01.wav  
2023/06/13  16:43         2,125,880 02.wav  
2023/06/13  16:44         1,908,280 03.wav  
2023/06/13  16:45         2,113,080 04.wav  
2023/06/13  16:47         2,714,680 05.wav  
2023/06/13  16:48         1,857,080 06.wav  
2023/06/13  16:49         1,729,080 07.wav  
2023/06/13  16:50         2,241,080 08.wav  
2023/06/13  16:50         1,959,480 09.wav  
2023/06/13  16:51         1,921,080 10.wav  
2023/06/13  16:52         1,921,080 11.wav  
2023/06/13  16:52         1,677,880 12.wav  
2023/06/13  17:00         1,754,680 13.wav  
2023/06/13  17:01         2,202,680 14.wav  
2023/06/13  17:01         2,023,480 15.wav  
2023/06/13  17:02         1,793,080 16.wav  
2023/06/13  17:03         2,586,680 17.wav  
2023/06/13  17:04         2,189,880 18.wav  
2023/06/13  17:04         2,573,880 19.wav  
2023/06/13  17:05         2,010,680 20.wav  
              20 个文件     40,213,600 字节  
               2 个目录 399,953,739,776 可用字节
```

当然，如果懒得准备训练集，也可以下载我切分好的，大家丰俭由己，各取所需：

```bash
链接：https://pan.baidu.com/s/1t5hE1LLktIPoyF70_GsH0Q?pwd=3dc6   
提取码：3dc6
```

至此，数据集就准备好了。

### 云端训练和推理

数据集准备好了，我们就可以进行训练了，在此之前，需要配置PaddlePaddle框架，但这一次，我们选择在云端直接进行训练，如果想要本地部署，请移步：[声音好听,颜值能打,基于PaddleGAN给人工智能AI语音模型配上动态画面(Python3.10)](https://v3u.cn/a_id_313)。

首先进入Paddle的云端项目地址：

```ruby
https://aistudio.baidu.com/aistudio/projectdetail/6384839
```

随后点击启动环境，注意这里尽量选择显存大一点的算力环境：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e5318b751a24f14aa10bf1679bd901e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

这里的机器有点类似Google的colab，原则上免费，通过消耗算力卡来进行使用。

成功启动环境之后，需要安装依赖：

```ini
# 安装实验所需环境  
!bash env.sh  
!pip install typeguard==2.13
```

由于机器是共享的，一旦环境关闭，再次进入还需要再次进行安装操作。

安装好paddle依赖后，在左侧找到文件 untitled.streamlit.py ，双击文件开启，随后点击web按钮，进入web页面。

接着在web页面中，点击Browse files按钮，将之前切分好的数据集上传到服务器内部。

接着点击检验数据按钮，进行数据集的校验。

最后输入模型的名称以及训练轮数，然后点击训练即可：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/494cb57f77974be797a7eccbccc7189b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

以TeslaV100为例子，20个文件的数据集200轮训练大概只需要五分钟就可以训练完毕。

模型默认保存在项目的checkpoints目录中，文件名称为master。

点击导出模型即可覆盖老的模型：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d3c5434c9c941cbbf131ffb6f5378e9~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

最后就是线上推理：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/16198b63cb2943f5b55af004b8c68c76~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

这里预制了三种声码器【PWGan】【WaveRnn】【HifiGan】, 三种声码器效果和生成时间有比较大的差距，这里推荐折中的PWGan声码器，因为毕竟是线上环境，每停留一个小时都会消耗算力点数。

合成完毕后，就可以拿到国师的克隆语音了。

### 结语

线上环境配置起来相对简单，但要记住，完成克隆语音任务后，需要及时关闭环境，防止算力点数的非必要消耗

## 批量生成,本地推理，人工智能声音克隆框架PaddleSpeech本地批量克隆实践(Python3.10)

云端炼丹固然是极好的，但不能否认的是，成本要比本地高得多，同时考虑到深度学习的训练相对于推理来说成本也更高，这主要是因为它需要大量的数据、计算资源和时间等资源，并且对超参数的调整也要求较高，更适合在云端进行。

在推理阶段，模型的权重和参数不再调整。相反，模型根据输入数据的特征进行计算，并输出预测结果。推理阶段通常需要较少的计算资源和时间，所以训练我们可以放在云端，而批量推理环节完全可以挪到本地，这样更适合批量的声音克隆场景。

### 本地配置PaddleSpeech

首先需要在本地安装PaddlePaddle框架，关于PaddlePaddle的本地配置，请移步：[声音好听,颜值能打,基于PaddleGAN给人工智能AI语音模型配上动态画面(Python3.10)-刘悦 (v3u.cn)](https://v3u.cn/a_id_313)

安装好PaddlePaddle之后，运行命令本地安装PaddleSpeech：

```bash
pip3 install paddlespeech
```

由于paddlespeech的依赖库中包括webrtcvad，如果本地环境没有安装过Microsoft Visual C++ 14.0，大概率会报这个错误：

```dart
building 'Crypto.Random.OSRNG.winrandom' extension  
error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual  
C++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools
```

此时需要安装一下Microsoft Visual C++ 14.0的开发者工具，最好不要使用微软的线上安装包，推荐使用离线安装包，下载地址：

```bash
链接：https://pan.baidu.com/s/1VSRHAMuDkhzQo7nM4JihEA?pwd=or7x   
提取码：or7x
```

安装完C++ 14.0即可完成PaddleSpeech的安装：

```python
D:\work\speech\master_voice>python  
Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] on win32  
Type "help", "copyright", "credits" or "license" for more information.  
>>> import paddlespeech  
>>>
```

### 下载音色模型和声码器

音色模型就是之前我们在：[声音克隆,精致细腻,人工智能AI打造国师“一镜到底”鬼畜视频,基于PaddleSpeech(Python3.10)](https://v3u.cn/a_id_316)中训练的国师的音色模型，下载地址：

```bash
链接：https://pan.baidu.com/s/1nKOPlI7P_u_a5UGdHX76fA?pwd=ygqp   
提取码：ygqp
```

随后下载声码器，这里推荐下载【PWGan】和【WaveRnn】两款声码器，不推荐【HifiGan】，因为【HifiGan】的效果实在太糟糕，PWGan的效果差强人意，WaveRnn质量最高，但推理时间也最慢。

下载地址：

```bash
链接：https://pan.baidu.com/s/1KHIZS5CrydtANXm6CszdYQ?pwd=6lsk   
提取码：6lsk
```

下载之后，分别解压到同一个目录即可。

### 本地推理

接下来我们就可以编写推理脚本了。

首先导入需要的模块：

```python
from pathlib import Path  
import soundfile as sf  
import os  
from paddlespeech.t2s.exps.syn_utils import get_am_output  
from paddlespeech.t2s.exps.syn_utils import get_frontend  
from paddlespeech.t2s.exps.syn_utils import get_predictor  
from paddlespeech.t2s.exps.syn_utils import get_voc_output  
  
# 音色模型的路径  
am_inference_dir = "./master"  
  
# 声码器的路径  
voc_inference_dir_pwgan = "./pwgan"   
  
# 声码器的路径  
voc_inference_dir_wavernn = "./wavernn"   
  
  
  
# 克隆音频生成的路径  
wav_output_dir = "./output"  
  
# 选择设备[gpu / cpu]，默认选择gpu，   
device = "gpu"
```

这里定义好模型和声码器的路径，同时定义输出路径，默认采用gpu进行推理，速度更快。

随后定义后要语音生成的文本：

```json
text_dict = {  
    "1": "我原来想拿中石油的offer",  
    "2": "是不是很大胆",  
    "3": "中石油",  
    "4": "国企天花板",  
    "5": "就是中石油",  
    "6": "出差可以逛太古里",  
    "7": "太爽了",  
    "8": "我最早准备面试的时候",  
    "9": "跟所有同学说的只面中石油",  
    "10": "所有的同学，包括亲戚，朋友，他们所有人很兴奋",  
    "11": "我女朋友也很兴奋",  
    "12": "中石油",  
    "13": "一直说的是去中石油",  
    "14": "我一直在做去中石油的准备",  
    "15": "当时我面试的时候",  
    "16": "我说试用期只要20天",  
    "17": "或者只要25天",  
    "18": "两周到三周",  
    "19": "hr说为什么?",  
    "20": "我说很简单",  
    "21": "我每天飞四川",  
    "22": "单程两个小时",  
    "23": "早上去一次",  
    "24": "晚上去一次",  
    "25": "每天去两次",  
    "26": "我坚持10天",  
    "27": "20次",  
    "28": "就是20次",  
    "29": "成都太古里",  
    "30": "哇简直太爽了",  
    "31": "逛街",  
    "32": "去10天就够了",  
    "33": "然后前面的十天在北京",  
    "34": "上班",  
    "35": "严格地上班",  
    "36": "我说试用期只要二十天",  
    "37": "咱试用期就结束了",  
    "38": "哇hr说真的太厉害",  
    "39": "就挑战性太大了",  
    "40": "一天都不能请假啊",  
    "41": "但是后来我还是放弃了，哈哈哈",  
  
  
    "42": "你知道为什么",  
    "43": "我研究了大量的员工去成都的案例",  
    "44": "嗯，也有一些基层员工",  
    "45": "还有尤其是最近一段时间一些比较大胆的行为",  
    "46": "就是牵手那个我也看了",  
    "47": "我专门看",  
    "48": "研究",  
    "49": "就一直，我就一直下不了决心",  
    "50": "其实我真的非常想去啊，内心深处非常想",  
    "51": "你知道最大问题是什么，当然这是一个专业问题，简单地说最大问题就是街拍",  
    "52": "就是街拍",  
    "53": "因为你去了他就拍你啊",  
    "54": "就没有办法",  
    "55": "对一个员工",  
    "56": "对一个向往太古里的员工",  
    "57": "一个经常逛太古里的员工来说",  
    "58": "他给你来一个街拍",  
    "59": "全给你拍下来",  
    "60": "上传抖音",  
    "61": "因为你不能蹭蹭蹭蹭",  
    "62": "逛的太快啊",  
    "63": "不能啊",  
    "64": "你从南边到北边",  
    "65": "你中间得逛啊",  
    "66": "就拍了",  
    "67": "就拍了",  
    "68": "第一是街拍避免不了",  
    "69": "无论怎么样",  
    "70": "我想来想去",  
    "71": "因为我算个内行嘛",  
    "72": "我不去了，我就知道街拍跑不了",  
    "73": "街拍，避免不了",  
  
    "74": "第二个",  
    "75": "你的工资会全都损失了",  
    "76": "不是损失一半的工资，一半无所谓",  
    "77": "是全部的工资，奖金，绩效，年终奖全都没有了",  
    "78": "然后你还得停职",  
    "79": "就很尴尬啊",  
    "80": "这样子就不好混了",  
    "81": "真的不好混了",  
    "82": "最后我差不多一个多月的思想斗争",  
    "83": "那是个重大决定",  
    "84": "因为我都是按照去中石油准备的",  
    "85": "背面试题呢",  
    "86": "后来说放弃",  
    "87": "我自己决定放弃",  
    "88": "一个人做的决定，一个人的思考",  
    "89": "一个多月以后我放弃了，我第一个电话打给人力，我说我放弃去中石油。他，啊这，就不能接受",  
    "90": "他已经完全沉浸到去太古里当中去了，你知道吧",  
    "91": "就想着太好了，就喜欢的不得了",  
    "92": "怎么可能就过来说服我",  
    "93": "我说你不用跟我说",  
    "94": "你都不太清楚",  
    "95": "反正去中石油",  
    "96": "说怎么可能，你能做到，就开始给我忽悠",  
    "97": "我放弃了",  
    "98": "然后我跟女朋友说放弃",  
    "99": "哎呀，她说她把包包裙子都买了，这那的",  
    "100": "所有人，大家都觉得太遗憾了。",  
    "101": "然后跟老板说",  
    "102": "最有意思是跟老板说",  
    "103": "说真的不去中石油了",  
    "104": "哎呀，哎呀",  
    "105": "就觉着好像就没劲了，哈哈哈",  
    "106": "说你不是开玩笑吧",  
    "107": "哎呀就觉得，好像不想要我了似的",  
    "108": "开玩笑啊，开玩笑",  
    "109": "就所有人都沮丧而失落",  
    "110": "就我看到大家的反应",  
    "111": "我也很难过，很难过",  
    "112": "我我，我后来还是放弃了",  
    "113": "放弃了，嗯",  
    "114": "所以中石油offer是一个学习",  
    "115": "它对于一个追求太古里的一个员工来说",  
    "116": "它是破坏性的",  
    "117": "你去了中石油又能怎么样呢?",  
    "118": "你丢掉了信仰",  
    "119": "丢掉了人格啊",  
    "120": "孰重孰轻啊",  
    "121": "所以我在学习",  
    "122": "我在学习做一个合格员工的思考",  
    "123": "这就是我的，遗憾",  
    "124": "但也许是我的一个清醒",  
    "125": "或者学习的心得",  
}
```

这里字典的key是文件名，value是音频的内容。

随后加载声码器地址中的配置文件：

```ini
# frontend  
frontend = get_frontend(  
    lang="mix",  
    phones_dict=os.path.join(am_inference_dir, "phone_id_map.txt"),  
    tones_dict=None  
)  
  
# am_predictor  
am_predictor = get_predictor(  
    model_dir=am_inference_dir,  
    model_file="fastspeech2_mix" + ".pdmodel",  
    params_file="fastspeech2_mix" + ".pdiparams",  
    device=device)  
  
# voc_predictor  
voc_predictor_pwgan = get_predictor(  
    model_dir=voc_inference_dir_pwgan,  
    model_file="pwgan_aishell3" + ".pdmodel",      
    params_file="pwgan_aishell3" + ".pdiparams",  
    device=device)  
  
  
voc_predictor_wavernn = get_predictor(  
    model_dir=voc_inference_dir_wavernn,  
    model_file="wavernn_csmsc" + ".pdmodel",      
    params_file="wavernn_csmsc" + ".pdiparams",  
    device=device)  
  
output_dir = Path(wav_output_dir)  
output_dir.mkdir(parents=True, exist_ok=True)  
  
sentences = list(text_dict.items())
```

这里我们准备两个声码器对象。

最后运行克隆函数：

```ini
def clone(voc_predictor):  
  
    merge_sentences = True  
    fs = 24000  
    for utt_id, sentence in sentences:  
        am_output_data = get_am_output(  
            input=sentence,  
            am_predictor=am_predictor,  
            am="fastspeech2_mix",  
            frontend=frontend,  
            lang="mix",  
            merge_sentences=merge_sentences,  
            speaker_dict=os.path.join(am_inference_dir, "phone_id_map.txt"),  
            spk_id=0, )  
        wav = get_voc_output(  
                voc_predictor=voc_predictor, input=am_output_data)  
        # 保存文件  
        sf.write(output_dir / (utt_id + ".wav"), wav, samplerate=fs)  
  
  
if __name__ == '__main__':  
      
    clone(voc_predictor_pwgan)
```

这里默认的采样率是24000，am模型使用fastspeech2_mix，因为它可以兼容英文的阅读。

声码器选择voc_predictor_pwgan，当然也可以将参数修改为voc_predictor_wavernn。

生成后的效果：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6213037ba2e845dcb03d804ff9d2517c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

### 结语2

基于声学模型 FastSpeech2的PaddleSpeech的产品力已经相当惊人，就算是放在全球人工智能领域的尺度上，摆在微软这种业界巨头的最佳产品Azure-tts旁边，也是毫不逊色的，感谢百度，让普通人也能玩恶搞配音项目，最后奉上国师的鬼畜视频一键生成项目：<https://github.com/zcxey2911/zhangyimou_voice_clone_text>
