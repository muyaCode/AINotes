# ChatGPT衍生的项目

GPT-4 内部技术的解密文档 ***GPT-4 Architecture,Infrastructure,Training Dataset, Costs,Vision, MoE***，披露了GPT-4 的架构、基础设施、训练数据集、成本、视觉 和 MoE 等关键信息。

过去几个月都陆续有一些关于 GPT-4 架构的猜测和爆料，这篇则是最为详细的解密 [**⋙ 阅读原文**](https:/www.semianalysis.com/p/gpt-4-architecture-infrastructure) | [**全文翻译(中文版)**](https:/mp.weixin.qq.com/s/AIwinPksV_u-RQfcoCD7nw)

> 需要注意的是，这是一篇付费订阅的文章，可以访问上方链接阅读中文翻译版本

## Claude 2 惊艳更新&免费使用，再也不担心被 OpenAI 封号啦

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a4b2bb64d044d0db5eb1690473722f3~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

7月11日，Anthropic 公司在官方推特发布了新一代AI模型 Claude 2，并开放**免费使用**。

> 链接：[claude.ai](https://claude.ai)
>
> 只对英国和美国的IP开放 (记得魔法开全局模式)

新版模型提升很大：

> 1. **支持更长的上下文**：最多支持 200K token (约 150K 个单词)，目前只开放了 100K
> 2. **生成更长的响应**：多达 4K token的连贯文档
> 3. **生成更好的代码**：在编程基准测试和人工反馈评估中表现显著提高
> 4. **使用了更多非英语数据**，以及2022年到2023年初的新数据
> 5. **支持上传和分析文本**：一次最多支持5个10M大小的 PDF、TXT、CSV 文件，并支持跨文件分析  [**⋙ Twitter @AnthropicAI**](https://twitter.com/AnthropicAI/status/1678759122194530304) | [**中文解读**](https://mp.weixin.qq.com/s/ovwPR78hCGUi8rjxulkrvw)

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf2c602dc503429aaab6d2d8985768ef~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

使用 Poe 也可以体验到 Claude 2 最新模型，从使用体验上来说与 Claude 网站保持一致。目前 Poe 内可以每天免费使用30次

> 链接：[poe.com/Claude-2-10…](https://poe.com/Claude-2-100k)
>
> 需要魔法，每天免费使用30次

## BibiGPT 哔哔终结者

[**BibiGPT**](https://b.jimmylv.cn/) **是一个基于gpt-3.5-turbo的**一键总结 B 站、油管、播客、本地文件的视频、音频内容网站。

BibiGPT操作方式非常简单，可以复制视频链接到该网站进行总结，也可以在原视频的基础上修改域名后缀即可快速总结。

BibiGPT还提供了部分设置，允许用户自定义输出内容格式。

BibiGPT的输出总结有**笔记视图**和**思维导图视图**，还允许**将总结一键导入Notion，快速整理笔记**。哇！这两个功能实在太爱了有木有！！！

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/29071e9c35db47a3bd7e78375cf245a2~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)



## ChatGPT Shortcut

[AiShort (ChatGPT Shortcut) - 简单易用的 ChatGPT 快捷指令表，让生产力倍增！标签筛选、关键词搜索和一键复制 Prompts | AiShort(ChatGPT Shortcut)-Tag filtering, keyword search, and one-click copy prompts](https://www.aishort.top/)是一个**ChatGPT提示词（prompt）的汇总网站**。

相信大家应该对[ChatGPT Prompt Examplesopen in new window](https://platform.openai.com/examples)、[Awesome ChatGPT Promptsopen in new window](https://github.com/f/awesome-chatgpt-prompts)、[Learn Prompting](https://learnprompting.org/)等项目比较熟悉了。能否将ChatGPT真正作为生产力工具去使用，而不仅是玩具，prompts的构建尤为重要。

因此，该网站的作者基于这些项目筛选了大量的优质prompts，根据领域和功能对其进行分类，添加针对提示词的**标签筛选**、**关键词搜索**、**一键复制**和**中英文切换**功能，创建了 ChatGPT Shortcut 项目，也是希望能够帮助人们更好的使用ChatGPT。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/db9bc743533c4b75978ba3bef17fa38d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

## 『飞书 ChatGPT』飞书×(GPT-3.5+DALL·E+Whisper)=飞一般的工作体验

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/df0f4aca4df24e87af2f716613a729e5~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

飞书ChatGPT，可以实现在飞书内的聊天机器人，完成语音对话、角色扮演、多话题讨论、图片创作、表格分析、文档导出、话题内容转PPT、与飞书文档互动功能，还可以查询 token 余额 & token 负载均衡。

项目给出了详细的多种部署方式教程。感觉很酷！可以走一波~[ConnectAI-E/Feishu-OpenAI: 🎒 飞书 ×（GPT-4 + DALL·E + Whisper）= 飞一般的工作体验 🚀 语音对话、角色扮演、多话题讨论、图片创作、表格分析、文档导出 🚀 (github.com)](https://github.com/ConnectAI-E/Feishu-OpenAI)

## 『ChatMind』根据文本提示生成思维导图，还支持表格和写作

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9be5f5555b324d4cbca159c7bf319070~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

ChatMind 是一个 AI 辅助工具，可以像操作 ChatGPT 一样，输入文本并生成对应主题的思维导图，可以实现一键演示、框架梳理、头脑风暴、项目管理、日程安排、笔记总结等多种功能，还支持一键保存。

此外，ChatMind 还支持与 AI 对话生成表格、处理表格数据、生成文字段落、，并提供了可以参考的写作模板，帮助更快地完成工作。

试了一下，对于给定具体一个书名的输入提示，ChatMind 生成的大纲跟实际内容对不上。不过，围绕某个话题给出的思维导图，还是很靠谱的，逻辑非常清晰。如图是根据“2023年读50本书”的目标给出的规划，还行！[ChatMind.Tech](https://www.chatmind.tech/)

## DebugCode.AI』免费的 AI 代码调试工具

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b25d155d75a146109fd5a68f14836083~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

DebugCode.AI 是一款强大且免费的 AI 代码调试工具，基于OpenAI API 智能分析和修复代码中的错误。你只需要粘贴代码、输入问题、然后点击 Debug 按钮，就可以得到一个修复方案！非常地简单和便捷，向手动调试的时代说再见！

官网：[debugcode.ai](https://debugcode.ai/)

## 『codeium AI代码工具』Copilot平替，个人永久免费使用，支持常用的编辑器

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c4c680aaf56410c9ae75b7caef6bad9~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

Codeium 是一个在线代码编辑器和编程辅助工具，可以理解为免费版的 GitHub Copilot。

Codeium 可以帮助开发者在不熟悉的语言或代码库进行修改，寻找和使用API，并且支持单元测试的自动生成，支持多种语言的自动补全，还提供了一个基于自然语言的全局查询能力。[Playground | Codeium · Free AI Code Completion & Chat](https://codeium.com/playground)

## 『亚马逊AI编程助手CodeWhisperer正式可用』面向个人开发者免费开放

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddc6e65be9cd4e0aa0b808d498a15f0d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

亚马逊云科技宣布，实时AI编程助手Amazon CodeWhisperer正式可用，同时推出的还有供所有开发人员免费使用的个人版（CodeWhisperer Individual）。

CodeWhisperer帮助开发者基于注释生成代码，追踪开源参考，扫描查找漏洞。此外，还可以帮助开发者创建代码胜任如下场景，比如常规、耗时的无差别任务，或是在使用不熟悉的API或SDK时构建示例代码等。[Amazon offers free access to its AI coding assistant to undercut Microsoft - The Verge](https://www.theverge.com/2023/4/13/23681796/amazon-ai-coding-assistant-codewhisperer-microsoft)

## HuggingChat

『HuggingFace发布ChatGPT开源替代品HuggingChat』公开向OpenAI闭源生态发起挑战

4月25日，Huggingface宣布推出HuggingChat，这是一款ChatGPT的开源替代品，可以与用户进行问答互动，目前还不支持中文回答。

HuggingFace相当于人工智能领域届的GitHub，基本上开源AI模型的Demo都跑在上面，因此很多人都开源的HuggingChat寄予厚望。不过，经过 ShowMeAI 社区小伙伴们的测试，HuggingChat与ChatGPT还存在较大差距，逊色于GPT-3.5。期待开源后大量用户对数据进行微调能提升体验。 [**在线体验**](https://huggingface.co/chat/)

## Character AI

月访问量2亿、首周170万安装量！声称碾压 ChatGPT

官网：<https://character.ai>

[Character.ai：每个人都可定制自己的个性化AI (baidu.com)](https://baijiahao.baidu.com/s?id=1763251679965944178&wfr=spider&for=pc)

## BLOOMChat: 开源可商用支持多语言的大语言模型

[逼近GPT-4！BLOOMChat: 开源可商用支持多语言的大语言模型 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg2MTcwNjc1Mg==&mid=2247484766&idx=1&sn=ccecd302b889a73b46c8d804f713bc25&chksm=ce124a31f965c32749d27391ecbfd185ae48f9a1e30aeed5d5fc2b0de97b3f06e9116d85e61b#rd)

## baichuan-7B: 开源可商用支持中英文的最好大模型

[baichuan-7B: 开源可商用支持中英文的最好大模型 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg2MTcwNjc1Mg==&mid=2247484799&idx=1&sn=b485d2cdff0e536436b19697c9d759bb&chksm=ce124a10f965c3061afd5d2919bc34665ee89b0d8abbabb88b434fc3a33282ca260f0d11ed86#rd)

## 轩辕：首个千亿级中文金融对话模型

[轩辕：首个千亿级中文金融对话模型 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg2MTcwNjc1Mg==&mid=2247484779&idx=1&sn=d983839154a79a9f0d21e327b5e9a557&chksm=ce124a04f965c3128b3a000182fe4e30c805540dd7bd8340602d749069744556658752c17f5a#rd)

## 剑桥/腾讯提出大语言模型 PandaGPT：一个模型统一六种模态

来自剑桥、NAIST 和腾讯 AI Lab 的研究者推出了一款名为 PandaGPT 的跨模态语言模型，展示了在人工智能领域的创新尝试。

通过结合 ImageBind 的模态对齐能力和 Vicuna 的生成能力，同时实现了六种模态下的指令理解与跟随能力。虽然 PandaGPT 的效果尚有提升空间，但展示了跨模态 AGI 智能的发展潜力 [**⋙ 中文解读**](https://mp.weixin.qq.com/s/YRbMZb2NCB6sJQZUFSCoug)

> *▢* 项目主页：[panda-gpt.github.io/](https://panda-gpt.github.io/)
>
> *▢* GitHub项目地址：[github.com/yxuansu/Pan…](https://github.com/yxuansu/PandaGPT)
>
> *▢* 论文：[arxiv.org/abs/2305.16…](https://link.juejin.cn?target=http%3A//arxiv.org/abs/2305.16355)
>
> *▢* 线上 Demo：[huggingface.co/spaces/GMFT…](https://huggingface.co/spaces/GMFTBY/PandaGPT)



作者：ShowMeAI
链接：https://juejin.cn/post/7241510305367998501
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。来自剑桥、NAIST 和腾讯 AI Lab 的研究者推出了一款名为 PandaGPT 的跨模态语言模型，展示了在人工智能领域的创新尝试。

通过结合 ImageBind 的模态对齐能力和 Vicuna 的生成能力，同时实现了六种模态下的指令理解与跟随能力。虽然 PandaGPT 的效果尚有提升空间，但展示了跨模态 AGI 智能的发展潜力 [**⋙ 中文解读**](https://mp.weixin.qq.com/s/YRbMZb2NCB6sJQZUFSCoug)

> *▢* 项目主页：[PandaGPT (panda-gpt.github.io)](https://panda-gpt.github.io/)
>
> *▢* GitHub项目地址：<https://github.com/yxuansu/PandaGPT>
>
> *▢* 论文：[2305.16355\] PandaGPT: One Model To Instruction-Follow Them All (arxiv.org)](https://arxiv.org/abs/2305.16355)
>
> *▢* 线上 Demo：[PandaGPT - a Hugging Face Space by GMFTBY](https://huggingface.co/spaces/GMFTBY/PandaGPT)

## 开源大语言模型

### 前言

OpenAI发布的ChatGPT火爆全球以来，全球互联网大厂陆续跟进，纷纷宣布了自家的Chat产品，如Google的Bard，百度的文心一言，阿里的通义千问等等。

这些Chat产品背后都是依赖的大语言模型(Large Language Model)。

如果是做一个垂直领域的Chat产品，有2种方案：

- 直接使用商业化产品，前提是商业化产品支持对模型做fine-tune(微调)。比如OpenAI就支持对它的基础模型做fine-tune来实现个性化的模型。
- 使用开源的大语言模型，对开源模型做fine-tune来实现垂直领域的Chat产品。

本文重点介绍有较大参考价值的开源大语言模型，方便大家快速找到适合自己应用场景的开源模型。

### 开源大语言模型

| Model      | 作者                                        | 参数量                                          | 训练数据量(tokens)                                      | 训练成本                                                     |
| :--------- | :------------------------------------------ | :---------------------------------------------- | :------------------------------------------------------ | :----------------------------------------------------------- |
| LLaMA      | Meta                                        | 包括 70 亿、130 亿、330 亿、650 亿 4 种参数规模 | 1.4万亿                                                 | 2048个A100 GPU                                               |
| Alpaca     | Stanford                                    | 70亿                                            | 52k条问答指令数据，指令数据来源于OpenAI的API返回结果    | 500美元数据成本+100美元训练成本                              |
| Vicuna     | UC Berkeley, CMU, Stanford, UCSD and MBZUAI | 130亿                                           | 70k条问答指令数据，指令数据来源于用户分享出来的对话记录 | 300美元                                                      |
| Koala      | UC Berkeley                                 | 130亿                                           | 500k条问答直录功能数据，指令数据来源于网上公开数据集    | 在公共云计算平台上，预期训练成本不超过100美元。一台 Nvidia DGX 服务器与8个A100 GPU，需要6个小时训练完成2个epochs。 |
| Dolly 2.0  | Databricks                                  | 120亿                                           | 15k条问答指令数据，指令数据来源于Databricks员工         | 不到30美元                                                   |
| ChatGLM    | 清华大学KEG 实验室和智谱AI                  | 60亿和1300亿共2种参数规模                       | 4000亿左右，中文和英文token各2000亿                     | 数百万人民币                                                 |
| 鹏程·盘古α | 鹏程实验室、华为                            | 26亿、130亿和2000亿共3种参数规模                | 2500亿                                                  | 2048 块昇腾处理器                                            |

开源模型有几个注意点：

- 第一，LLaMA由Meta开源，LLaMA目前仅用于学术、社会公益项目，不能用于商业化项目。
- 第二，Alpaca, Vicuna, Koala基于LLaMA衍生而来，是在LLaMA大语言模型基础上做了fine-tune得到的，因此训练成本极低，只需用比较少的指令数据做fine-tune即可。这也是为什么这几个模型的训练成本很低，因为站在了LLaMA这个巨人的肩膀上。另外，这几个模型由于本质上还是LLaMA，受限于LLaMA的license限制，同样不能用于商业化目的。
- Dolly 2.0是在EleutherAI pythia模型衍生而来，指令微调的数据集称为 databricks-dolly-15k，也已开源发布，包含来自数千名 Databricks 员工的 15,000 个高质量的人工生成的问答数据，专为指令调优大型语言模型而设计。且 databricks-dolly-15k 根据（Creative Commons Attribution-ShareAlike 3.0 Unported License）的许可条款，任何人都可以出于任何目的使用、修改或扩展此数据集，包括商业应用。
- 国内目前开源的主要就是清华主导的ChatGLM，以及华为和鹏程实验室主导的盘古alpha模型。

### 训练模型

如果拿大语言模型做训练，而不是简单的指令微调，那训练成本非常高昂，比如ChatGPT训练一次的成本在140万美元左右。

最近微软开源了DeepSpeed，可以加速大语言模型的训练，将ChatGPT 1750亿参数模型的训练成本降低到5120美元左右。

其本质是一个开源深度学习训练优化库，可以加速ChatGPT模型的训练，比目前最快的训练方法快大约15倍，如果想自己训练大语言模型的可以参考下。

### 总结

GPT模型现在真的是日新月异，很多是基于基础模型，结合问答的指令数据对模型做微调而得到的。

现在很多媒体报道的时候喜欢夸大，大家不要看到冒出一个新的开源模型就觉得多么厉害了，绝大部分都是站在巨人肩膀上做了微调而来的。

上面开源大语言模型的表格也会持续更新，欢迎大家关注下面的开源地址。

### 开源地址

持续更新的开源大语言模型开源地址：ChatGPT模型教程。

公众号：coding进阶。

个人网站：Jincheng's Blog。

知乎：无忌。

### References

- https://mp.weixin.qq.com/s/7CW4p8RgAF3jYGmgefB_eg
- https://mp.weixin.qq.com/s/M-ToNk8SABoP2JG0xLUBxQ

## 用fastgpt打造专属知识库

用fastgpt打造专属知识库，可自行部署到服务器或集群

官网：[Fast GPT](https://ai.fastgpt.run/)

开源地址：<https://github.com/c121914yu/FastGPT>

【用fastgpt打造专属知识库，可自行部署到服务器或集群】https://www.bilibili.com/video/BV1wu411e75C?vd_source=36c9491a7fa2ab8a22ca060af01b7472

## Albus自动生成图文知识图谱

Albus官网：<https://www.albus.org/>

完全免费，结合了ChatGPT和Obsidian的功能，可以作为你的专属知识探索引擎，帮你快速出视觉化的知识图谱涵盖各个领域

- 【比思维导图更强大！三分钟学会用GPT自动生成图文知识图谱学遍一切】<https://www.bilibili.com/video/BV1mc41137Bu?vd_source=36c9491a7fa2ab8a22ca060af01b7472>
- 【《ChatGPT自学的正确打开方式 》 小学、初中到成人教育全覆盖】<https://www.bilibili.com/video/BV1h14y127ND?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

## ChatRWKV模型6G显存部署实战

ChatRWKV模型6G显存部署实战，可极限操作1.5GB显存部署14B模型

- 1、项目Git：github.com/BlinkDL/ChatRWKV
- 2、中文在线：modelscope.cn/studios/BlinkDL/RWKV-CHN/summary/
- 3、Raven英语14B在线：huggingface.co/spaces/BlinkDL/ChatRWKV-gradio
- 4、ChatRWKV LoRA微调：github.com/Blealtan/RWKV-LM-LoRA
- 5、ChatRWKV C++：github.com/harrisonvanderbyl/rwkv-cpp-cuda
- 6、Wenda：github.com/l15y/wenda
- 7、模型量化：Use v2/convert_model.py to convert a model for a strategy, for faster loading & saves CPU RAM.
- 8、作者给出的中文教程：zhuanlan.zhihu.com/p/618011122
- 9、1.5GB显存部署14B模型：zhuanlan.zhihu.com/p/616986651

部署视频教程：【ChatRWKV模型6G显存部署实战—凡人之躯, 比肩ChatGPT!!?】<https://www.bilibili.com/video/BV1hm4y1C7Ai?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

## RWKV-Runner可商用的大语言模型

RWKV-Runner发布并开源，可商用的大语言模型，一键启动管理，2-32G显存适配，API兼容，一切前端皆可用

开源仓库地址：<https://github.com/josStorer/RWKV-Runner>

下载地址（RWKV目录）：<https://pan.baidu.com/s/1wchIUHgne3gncIiLIeKBEQ?pwd=1111>

RWKV官方仓库：<https://github.com/BlinkDL/RWKV-LM>

【RWKV-Runner发布并开源，可商用的大语言模型，一键启动管理，2-32G显存适配，API兼容，一切前端皆可用】<https://www.bilibili.com/video/BV1hM4y1v76R?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

## DeepGPT

视频教程：【DeepGPT，可自己部署的类agentGPT/AutoGPT工具，纯前端更轻量，国内可用】<https://www.bilibili.com/video/BV1As4y1k73M?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

官方仓库：<https://github.com/easychen/deepgpt-dist>

在线版本：<https://d.level06.com> (如果无法访问，请到官方仓库查看新域名)

独立部署版下载：<https://github.com/easychen/deepgpt-dist/build.zip>

## 使用 Chato 搭建自己的知识库 | 轻松定制 ChatGPT 智能问答机器人

体验地址：<https://chato.cn/>

Chato是一款强大而易用的工具，能帮助你将垂直行业知识注入AI，并创建属于自己的AI助手。无需编程，通过上传文档或回答问题，Chato自动学习并建立知识库。你可以设定机器人角色、定制个性，并灵活发布到各渠道。

## ChatGLM-6B开源GPT加上自己的知识库

【开源GPT加上自己的知识库比ChatGTP更精准更专业】<https://www.bilibili.com/video/BV19o4y1J7mL?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

开源地址：[THUDM/ChatGLM-6B: ChatGLM-6B: An Open Bilingual Dialogue Language Model | 开源双语对话语言模型 (github.com)](https://github.com/THUDM/ChatGLM-6B)

## 『搭建基于LLM的客服系统的实践』基于清华大模型 ChatGLM-6B 完成6类任务

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4fdf4f60019749b69048e2b4d4d43452~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

随着 ChatGPT 和 GPT-4 等强大生成模型出现，自然语言处理任务方式正在逐步发生改变：或许未来我们不再为每个具体任务去 finetune 一个模型，而是基于同一个大模型对不同任务设计其独有的 prompt，以解决不同的问题。

因此，作者基于清华开源大模型 ChatGLM-6B，构建了一个公司的客服系统，并使用代码完成以下6个任务，来展示详细过程。

> *▢* 任务1：客服打招呼任务 (已完成)
>
> *▢* 任务2：实现交谈任务分类 (已完成)
>
> *▢* 任务3：实现交谈内容相似度任务 (已完成)
>
> *▢* 任务4：实现交谈内容结构化提取任务
>
> *▢* 任务5：实现本地知识库和网络搜索的增强任务
>
> *▢* 任务6：实现自动化任务

### 🔔 系列简介

> *0*. 环境说明
>
> *1*. ChatGLM-6B介绍
>
> *2*. 硬件需求：最低 GPU 显存
>
> *3*. 环境安装：使用 pip 安装依赖
>
> *4*. API部署 [**⋙ 原文**](https://zhuanlan.zhihu.com/p/626232785)

### 🔔 任务1：客服打招呼任务

> *1*. 首先定义role的prompt，让模型知道自己的角色
>
> *2*. 然后再描述一下客户的基本信息
>
> *3*. 最后加上命令，生成完整的prompt，发给模型 [**⋙ 原文**](https://zhuanlan.zhihu.com/p/626234226)

### 🔔 任务2：实现交谈任务分类

> *1*. 根据场景定义好分类
>
> *2*. 按照业务场景预先生成分类的例子
>
> *3*. 构建 prompt 作为 pre_history 送到模型参数中
>
> *4*. 把要做分类的句子输给模型，模型根据要求进行分类，并输出 [**⋙ 原文**](https://zhuanlan.zhihu.com/p/626416551)

### 🔔 任务3：实现交谈内容相似度任务

> *1*. 根据场景选择一些常用的对话
>
> *2*. 按照业务场景预先生成对话比对的例子 (比对目标最好是内设场景对话)
>
> *3*. 构建 prompt 作为 pre_history 送到模型参数中
>
> *4*. 把要做分类的句子输给模型，模型根据要求进行比对，并输出是还是不是 [**⋙ 原文**](https://zhuanlan.zhihu.com/p/627333923)

目前，作者发布了3个任务的实现细节，ShowMeAI将与大家共同关注系列更新进展。

## langchain+chatGLM的本地知识库项目

教程：【【防坑指南】手把手演示本机部署langchain+chatGLM本地知识库】<https://www.bilibili.com/video/BV1Ah4y1d79a?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

传送门：<https://github.com/imClumsyPanda/langchain-ChatGLM>

Torch版本地址：<https://download.pytorch.org/whl/torch_stable.html>

关于硬件要求方面，作者文档里提到了embedding模型需要使用3G显存，chatglm最低需要6G显存。

## CodeWhisperer AI编程工具

CodeWhisperer官网注册链接：<https://aws.amazon.com/cn/codewhisperer/?trk=a2076b82-2c5d-475a-8b78-f22f4bb4f9a1&sc_channel=display+ads>

2023年度亚马逊云科技中国峰会报名：<https://summit.awsevents.cn/2023/form.html?source=A1Jmo+hFUc3p2QU//2KWmogg5HLpq2+1rdi8UDFl+>

【【AI编程】秒杀LeetCode！以后程序员只用写注释？CodeWhisperer安装使用教程】<https://www.bilibili.com/video/BV1Sg4y1K7iD?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

## chatgpt engineer AI编程工具



## Github Copilot



## ChatGPT实时语音聊天

ChatGPT+VITS+Galgame

【ChatGPT+Galgame 与老婆自由对话！】<https://www.bilibili.com/video/BV1TD4y1E7e8?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

项目地址：<https://github.com/cjyaddone/ChatWaifu>
L2D版本：<https://github.com/cjyaddone/ChatWaifuL2D>
QQ机器人版本：<https://github.com/MuBai-He/ChatWaifu-marai>

## CyberWaifu开源聊天机器人 hatGPT+Claude

【【CyberWaifu开源发布，ChatGPT+Claude】我要给她完整的一生，引入思考链和记忆数据库】<https://www.bilibili.com/video/BV11V4y1z7rS?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

开源地址：<https://github.com/Syan-Lin/CyberWaifu>

## GPT4All：ChatGPT本地私有化部署,终生免费

# GPT4All是什么

根据官方网站GPT4All的描述，它是一个免费使用、本地运行的、注重隐私的聊天机器人。不需要GPU或互联网。

GTP4All是一个生态系统，用于训练和部署在消费级CPU上本地运行的强大且定制的大型语言模型。

我们的GPT4All模型是一个4GB的文件，您可以下载并连接到GPT4All开源生态系统软件中。Nomic AI提供高质量和安全的软件生态系统，努力实现个人和组织轻松训练和本地实施自己的大型语言模型。

# 它是如何工作的？

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d0edccfe4c94488a938806a5c6af8561~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

该过程非常简单（当您了解之后），并且可以在其他模型上重复。具体步骤如下：

- 加载GPT4All模型
- 使用Langchain检索并加载我们的文档
- 将文档分割成小块，以便嵌入式能够理解
- 使用FAISS根据我们要传递给GPT4All的问题创建嵌入式向量数据库
- 在基于问题的语境中在嵌入式向量数据库上执行相似性搜索（语义搜索）：这将作为我们问题的上下文
- 使用Langchain将问题和上下文提供给GPT4All，并等待答案。

所以我们需要的是嵌入式向量。嵌入式向量是一条信息的数值表示，例如文本、文档、图像、音频等。该表示捕捉了被嵌入的语义含义，这正是我们所需要的。对于这个项目，我们无法依赖于重型GPU模型：因此，我们将下载Alpaca原生模型，并使用Langchain中的LlamaCppEmbeddings。不用担心！每一步都有详细的解释。

# Let’s start coding

**创建虚拟环境**

为你的新Python项目创建一个新的文件夹，例如GPT4ALL_Fabio（请用你的名字替换Fabio）：

```python
mkdir GPT4ALL_Fabiocd GPT4ALL_Fabio
```

接下来，创建一个新的Python虚拟环境。如果你安装了多个Python版本，请指定你想要使用的版本。在这个例子中，我将使用与Python 3.10关联的主要安装。

```python
python3 -m venv .venv
```

命令python3 -m venv .venv创建了一个名为.venv的新虚拟环境（点号会创建一个名为venv的隐藏目录）。

虚拟环境提供了一个隔离的Python安装环境，允许你仅针对特定项目安装软件包和依赖项，而不会影响系统范围的Python安装或其他项目。这种隔离有助于保持一致性，并防止不同项目需求之间的潜在冲突。

一旦虚拟环境创建好了，你可以使用以下命令来激活它：

```python
source .venv/bin/activate
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/90a7cf0ca66a4c2ab897e8b8f3550695~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

**安装依赖**

对于我们正在构建的项目，我们不需要太多的软件包。我们只需要以下几个：

- GPT4All的Python绑定
- Langchain用于与我们的文档交互

```python
pip install pygpt4all==1.0.1
pip install pyllamacpp==1.0.6
pip install langchain==0.0.149
pip install unstructured==0.6.5
pip install pdf2image==1.16.3
pip install pytesseract==0.3.10
pip install pypdf==3.8.1
pip install faiss-cpu==1.7.4
```

对于LangChain，你可以看到我们指定了版本号。这个库最近正在接受很多更新，所以为了确保我们的设置明天也能正常工作，最好指定一个我们知道能正常工作的版本。Unstructured是pdf加载器、pytesseract和pdf2image的必需依赖库。

注意：GitHub存储库中有一个requirements.txt文件，其中包含与此项目相关的所有版本。你可以在将其下载到主项目文件目录后，使用以下命令一次性进行安装：

```python
pip install -r requirements.txt
```

请记住，某些库有根据你在虚拟环境中运行的Python版本提供的不同版本可用。

**下载模型**

这是一个非常重要的步骤。

对于这个项目，我们当然需要

GPT4All模型。在Nomic AI网站上描述的过程非常复杂，并且需要一些我们不一定都拥有的硬件（比如我）。所以这里是已经转换并准备好使用的模型链接。只需点击下载。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c471d7c9ddb847acb3d4f827f17db80b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

正如简介中简要描述的，我们还需要嵌入模型，这是一个可以在我们的CPU上运行而不会崩溃的模型。点击这里的链接【[huggingface.co/Pi3141/alpa…](https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FPi3141%2Falpaca-native-7B-ggml%2Fblob%2F397e872bf4c83f4c642317a5bf65ce84a105786e%2Fggml-model-q4_0.bin%E3%80%91%E4%B8%8B%E8%BD%BD%E5%B7%B2%E7%BB%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA4%E4%BD%8D%E5%B9%B6%E5%87%86%E5%A4%87%E5%A5%BD%E7%94%A8%E4%BD%9C%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84alpaca-native-7B-ggml%E6%A8%A1%E5%9E%8B%E3%80%82)

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00c538205a9d4112a8e7dc0d69150a8b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

为什么我们需要嵌入？如果你还记得流程图中的第一步，在收集知识库文档之后，我们需要将它们嵌入。LLamaCPP嵌入来自Alpaca模型，非常适合这项工作，而且这个模型也很小（4 GB）。顺便说一下，你也可以在问答环节中使用Alpaca模型！

2023.05.25更新：Mani Windows用户在使用llamaCPP嵌入时遇到了问题。这主要是因为在安装python包llama-cpp-python时使用了以下命令：

```python
pip install llama-cpp-python
```

这个命令会从源代码编译库。通常Windows上的机器默认没有安装CMake或C编译器。但不要担心，有解决办法。

在Windows上运行llama-cpp-python的安装过程时，需要编译源代码，但由于Windows默认没有安装CMake和C编译器，因此无法从源代码构建。

在Mac用户使用Xtools和Linux用户上，通常操作系统中已经安装了C编译器。

**为了避免问题，你必须使用预编译的wheel。**

访问[github.com/abetlen/lla…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fabetlen%2Fllama-cpp-python%2Freleases)

找到适用于你架构和Python版本的预编译的wheel版本，你必须选择0.1.49版本，因为更高的版本不兼容。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/60043a314f5a45f0a4c5c4007170e894~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

在我这里，我使用的是Windows 10 64位，Python 3.10

所以我的文件是llama_cpp_python-0.1.49-cp310-cp310-win_amd64.whl

这个问题在GitHub存储库中有记录【[github.com/fabiomatric…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffabiomatricardi%2FGPT4All_Medium%2Fissues%2F2%E3%80%91%E3%80%82)

下载完成后，你需要将这两个模型文件放到models目录中，如下所示。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/457d42bb3b354731a7ef653d0360a459~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

目录结构和放置模型文件的位置

# 与GPT4All的基本交互

由于我们想要对GPT模型的交互进行控制，我们需要创建一个Python文件（我们称之为pygpt4all_test.py），导入依赖项并给模型发送指令。你会发现这很容易。

```python
from pygpt4all.models.gpt4all import GPT4All
```

这是我们模型的Python绑定。现在我们可以调用它并开始提问。让我们试一个有创意的问题。

我们创建一个函数来读取模型的回调，并要求GPT4All完成我们的句子。

```python
def new_text_callback(text):
    print(text, end="")

model = GPT4All('./models/gpt4all-converted.bin')
model.generate("Once upon a time, ", n_predict=55, new_text_callback=new_text_callback)
```

第一条语句告诉我们的程序在哪里找到模型（记住我们在上面的部分所做的事情）。

第二条语句要求模型生成一个回答，并完成我们的提示文本 "Once upon a time,"。

要运行它，请确保虚拟环境仍处于激活状态，然后简单地运行：

```python
python3 pygpt4all_test.py
```

你应该会看到模型正在加载的文本和句子的完成。根据你的硬件资源，这可能需要一些时间。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a37ac07d88d04e84a13b2138090638cb~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

结果可能与你的不同... 但对我们来说重要的是它正在工作，我们可以继续使用LangChain创建一些高级功能。

注意（更新于2023.05.23）：如果你遇到与pygpt4all相关的错误，请查看故障排除部分，其中提供了由Rajneesh Aggarwal或Oscar Jeong给出的解决方案。

# LangChain在GPT4All上的模板

LangChain框架是一个非常强大的库。它提供了组件，以一种易于使用的方式与语言模型进行交互，并且还提供了Chains。可以将Chains视为以特定方式组装这些组件，以最好地完成特定的用例。它们旨在成为一个更高级的接口，使人们能够轻松地开始使用特定的用例。这些Chains也可以进行定制。

在我们的下一个Python测试中，我们将使用一个Prompt Template。语言模型接受文本作为输入，这段文本通常被称为prompt。通常情况下，这不仅仅是一个硬编码的字符串，而是模板、示例和用户输入的组合。LangChain提供了多个类和函数，使构建和处理prompt变得简单。让我们看看如何实现。

创建一个新的Python文件，命名为my_langchain.py。

```python
# Import of langchain Prompt Template and Chain
from langchain import PromptTemplate, LLMChain

# Import llm to be able to interact with GPT4All directly from langchain
from langchain.llms import GPT4All

# Callbacks manager is required for the response handling 
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

local_path = './models/gpt4all-converted.bin' 
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
```

我们从LangChain中导入了Prompt Template和Chain，以及GPT4All llm类，以便能够直接与我们的GPT模型进行交互。

然后，在设置了llm路径之后（与之前一样），我们实例化了回调管理器，以便能够捕获我们查询的响应。

创建一个模板非常简单：根据文档教程，我们可以使用如下代码：

```python
template = """Question: {question}

Answer: Let's think step by step on it.

"""
prompt = PromptTemplate(template=template, input_variables=["question"])
```

template变量是一个多行字符串，包含了与模型的交互结构：在花括号中插入模板的外部变量，对于我们的情况就是问题。

由于它是一个变量，你可以决定它是一个硬编码的问题还是用户输入的问题：这里是两个示例。

```python
# Hardcoded question
question = "What Formula 1 pilot won the championship in the year Leonardo di Caprio was born?"

# User input question...
question = input("Enter your question: ")
```

对于我们的测试运行，我们将注释掉用户输入的问题。现在我们只需要将我们的模板、问题和语言模型连接在一起。

```python
template = """Question: {question}
Answer: Let's think step by step on it.
"""

prompt = PromptTemplate(template=template, input_variables=["question"])

# initialize the GPT4All instance
llm = GPT4All(model=local_path, callback_manager=callback_manager, verbose=True)

# link the language model with our prompt template
llm_chain = LLMChain(prompt=prompt, llm=llm)

# Hardcoded question
question = "What Formula 1 pilot won the championship in the year Leonardo di Caprio was born?"

# User imput question...
# question = input("Enter your question: ")

#Run the query and get the results
llm_chain.run(question)
```

记得验证你的虚拟环境仍然处于激活状态，并运行以下命令：

```python
python3 my_langchain.py
```

你可能会得到与我不同的结果。令人惊奇的是，你可以看到GPT4All在尝试为你找到答案时所遵循的整个推理过程。调整问题可能会得到更好的结果。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d45a22fb86ef42c4a4c84951d16b96e7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

# 使用LangChain和GPT4All回答关于文件的问题

在这里，我们开始了令人惊奇的部分，因为我们将使用GPT4All作为一个聊天机器人来回答我们的问题。

根据与GPT4All进行问答的工作流程，我们需要加载我们的PDF文件，并将其分成块。接下来，我们需要为我们的嵌入向量准备一个向量存储库。我们需要将我们的分块文档输入到向量存储库中进行信息检索，然后将它们与该数据库上的相似性搜索一起作为LLM查询的上下文进行嵌入。

为此，我们将直接使用Langchain库中的FAISS。FAISS是Facebook AI Research开发的开源库，旨在快速在大规模高维数据集中查找相似项。它提供索引和搜索方法，使得在数据集中快速找到最相似的项变得更加简单和快速。对我们来说特别方便的是，它简化了信息检索，并允许我们本地保存创建的数据库：这意味着在第一次创建后，将非常快速地加载数据库以供以后使用。

**创建向量索引数据库**

创建一个新的文件叫做 my_knowledge_qna.py：

```python
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# function for loading only TXT files
from langchain.document_loaders import TextLoader

# text splitter for create chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

# to be able to load the pdf files
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader

# Vector Store Index to create our database about our knowledge
from langchain.indexes import VectorstoreIndexCreator

# LLamaCpp embeddings from the Alpaca model
from langchain.embeddings import LlamaCppEmbeddings

# FAISS  library for similaarity search
from langchain.vectorstores.faiss import FAISS

import os  #for interaaction with the files
import datetime
```

第一组库与之前使用的相同：另外我们使用Langchain进行向量存储索引的创建，使用LlamaCppEmbeddings与我们的Alpaca模型进行交互（使用cpp库进行4位量化和编译），以及PDF加载器。

我们还要加载具有自己路径的LLMs：一个用于嵌入和一个用于文本生成。

```python
# assign the path for the 2 models GPT4All and Alpaca for the embeddings 
gpt4all_path = './models/gpt4all-converted.bin' 
llama_path = './models/ggml-model-q4_0.bin' 
# Calback manager for handling the calls with  the model
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# create the embedding object
embeddings = LlamaCppEmbeddings(model_path=llama_path)
# create the GPT4All llm object
llm = GPT4All(model=gpt4all_path, callback_manager=callback_manager, verbose=True)
```

为了测试，让我们看看是否成功读取了所有的PDF文件：第一步是声明三个函数，用于处理每个单独的文档。第一个函数是将提取的文本分割成块，第二个函数是创建带有元数据的向量索引（例如页码等），最后一个函数是用于测试相似性搜索（稍后我将更详细地解释）。

```python
# Split text 
def split_chunks(sources):
    chunks = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)
    for chunk in splitter.split_documents(sources):
        chunks.append(chunk)
    return chunks


def create_index(chunks):
    texts = [doc.page_content for doc in chunks]
    metadatas = [doc.metadata for doc in chunks]

    search_index = FAISS.from_texts(texts, embeddings, metadatas=metadatas)

    return search_index


def similarity_search(query, index):
    # k is the number of similarity searched that matches the query
    # default is 4
    matched_docs = index.similarity_search(query, k=3) 
    sources = []
    for doc in matched_docs:
        sources.append(
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }
        )

    return matched_docs, sources
```

现在我们可以测试文档目录中的索引生成：我们需要将所有的PDF文件放在那里。Langchain还有一种加载整个文件夹的方法，不论文件类型如何：由于后续处理比较复杂，我将在下一篇关于LaMini模型的文章中介绍。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/754c80dd8feb456faf5e2229109b1d2d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

我们将把这些函数应用到列表中的第一个文档上。

```python
# get the list of pdf files from the docs directory into a list  format
pdf_folder_path = './docs'
doc_list = [s for s in os.listdir(pdf_folder_path) if s.endswith('.pdf')]
num_of_docs = len(doc_list)
# create a loader for the PDFs from the path
loader = PyPDFLoader(os.path.join(pdf_folder_path, doc_list[0]))
# load the documents with Langchain
docs = loader.load()
# Split in chunks
chunks = split_chunks(docs)
# create the db vector index
db0 = create_index(chunks)
```

在第一行中，我们使用os库来获取docs目录中的PDF文件列表。然后，我们使用Langchain加载docs文件夹中的第一个文档（doc_list[0]），将其分割成块，然后使用LLama嵌入创建向量数据库。

正如您所看到的，我们使用了pyPDF方法。这个方法稍微复杂一些，因为您需要逐个加载文件，但是使用pypdf将PDF加载到文档数组中使您能够得到一个数组，其中每个文档都包含页面内容和带有页码的元数据。当您想要知道我们将通过查询提供给GPT4All的上下文的来源时，这非常方便。以下是来自readthedocs的示例：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2aa07f0d961847988dfa35283ef3eddf~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

然后执行下面的命令运行：

```python
python3 my_knowledge_qna.py
```

在加载用于嵌入的模型之后，您将看到令牌在索引中的工作方式：不要惊慌，因为这需要时间，特别是如果您只在CPU上运行，就像我一样（这需要8分钟）。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ab58d1442f91473999f9870892357c43~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

正如我之前解释的，pyPDF方法较慢，但为我们提供了用于相似性搜索的额外数据。为了遍历所有文件，我们将使用FAISS中的一种便捷方法，它允许我们将不同的数据库合并在一起。现在我们做的是使用上面的代码生成第一个数据库（我们将其称为db0），然后使用for循环创建列表中下一个文件的索引，并立即将其与db0合并。

以下是代码：请注意，我添加了一些日志，以便使用datetime.datetime.now()提供进度状态，并打印结束时间和开始时间的差值来计算操作所需的时间（如果您不喜欢，可以将其删除）。

合并指令如下：

```python
# merge dbi with the existing db0db0.merge_from(dbi)
```

最后一条指令是将我们的数据库保存到本地：整个生成过程可能需要数小时（取决于文档的数量），所以我们只需执行一次这个操作非常好！

```python
# Save the databasae locallydb0.save_local("my_faiss_index")
```

以下是完整的代码。当我们与GPT4All互动并直接从文件夹加载索引时，我们将对其中许多部分进行注释。

```python
# get the list of pdf files from the docs directory into a list  format
pdf_folder_path = './docs'
doc_list = [s for s in os.listdir(pdf_folder_path) if s.endswith('.pdf')]
num_of_docs = len(doc_list)
# create a loader for the PDFs from the path
general_start = datetime.datetime.now() #not used now but useful
print("starting the loop...")
loop_start = datetime.datetime.now() #not used now but useful
print("generating fist vector database and then iterate with .merge_from")
loader = PyPDFLoader(os.path.join(pdf_folder_path, doc_list[0]))
docs = loader.load()
chunks = split_chunks(docs)
db0 = create_index(chunks)
print("Main Vector database created. Start iteration and merging...")
for i in range(1,num_of_docs):
    print(doc_list[i])
    print(f"loop position {i}")
    loader = PyPDFLoader(os.path.join(pdf_folder_path, doc_list[i]))
    start = datetime.datetime.now() #not used now but useful
    docs = loader.load()
    chunks = split_chunks(docs)
    dbi = create_index(chunks)
    print("start merging with db0...")
    db0.merge_from(dbi)
    end = datetime.datetime.now() #not used now but useful
    elapsed = end - start #not used now but useful
    #total time
    print(f"completed in {elapsed}")
    print("-----------------------------------")
loop_end = datetime.datetime.now() #not used now but useful
loop_elapsed = loop_end - loop_start #not used now but useful
print(f"All documents processed in {loop_elapsed}")
print(f"the daatabase is done with {num_of_docs} subset of db index")
print("-----------------------------------")
print(f"Merging completed")
print("-----------------------------------")
print("Saving Merged Database Locally")
# Save the databasae locally
db0.save_local("my_faiss_index")
print("-----------------------------------")
print("merged database saved as my_faiss_index")
general_end = datetime.datetime.now() #not used now but useful
general_elapsed = general_end - general_start #not used now but useful
print(f"All indexing completed in {general_elapsed}")
print("-----------------------------------")
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2f20a1e72f104591928707a44402e45f~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

**向GPT4All提问关于您的文档的问题**

现在我们到了这一步。我们有了我们的索引，我们可以加载它，并使用一个提示模板向GPT4All提问。我们先从一个硬编码的问题开始，然后循环遍历我们的输入问题。

将以下代码放入一个名为 db_loading.py 的 Python 文件中，并在终端中使用 python3 db_loading.py 命令运行它:

```python
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# function for loading only TXT files
from langchain.document_loaders import TextLoader
# text splitter for create chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter
# to be able to load the pdf files
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader
# Vector Store Index to create our database about our knowledge
from langchain.indexes import VectorstoreIndexCreator
# LLamaCpp embeddings from the Alpaca model
from langchain.embeddings import LlamaCppEmbeddings
# FAISS  library for similaarity search
from langchain.vectorstores.faiss import FAISS
import os  #for interaaction with the files
import datetime

# TEST FOR SIMILARITY SEARCH

# assign the path for the 2 models GPT4All and Alpaca for the embeddings 
gpt4all_path = './models/gpt4all-converted.bin' 
llama_path = './models/ggml-model-q4_0.bin' 
# Calback manager for handling the calls with  the model
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# create the embedding object
embeddings = LlamaCppEmbeddings(model_path=llama_path)
# create the GPT4All llm object
llm = GPT4All(model=gpt4all_path, callback_manager=callback_manager, verbose=True)

# Split text 
def split_chunks(sources):
    chunks = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)
    for chunk in splitter.split_documents(sources):
        chunks.append(chunk)
    return chunks


def create_index(chunks):
    texts = [doc.page_content for doc in chunks]
    metadatas = [doc.metadata for doc in chunks]

    search_index = FAISS.from_texts(texts, embeddings, metadatas=metadatas)

    return search_index


def similarity_search(query, index):
    # k is the number of similarity searched that matches the query
    # default is 4
    matched_docs = index.similarity_search(query, k=3) 
    sources = []
    for doc in matched_docs:
        sources.append(
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }
        )

    return matched_docs, sources

# Load our local index vector db
index = FAISS.load_local("my_faiss_index", embeddings)
# Hardcoded question
query = "What is a PLC and what is the difference with a PC"
docs = index.similarity_search(query)
# Get the matches best 3 results - defined in the function k=3
print(f"The question is: {query}")
print("Here the result of the semantic search on the index, without GPT4All..")
print(docs[0])
```

打印的文本是与查询最匹配的前3个来源的列表，还提供了文档名称和页码。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e59caacc4afa49dba6d614e9ce54326d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

现在我们可以使用相似性搜索作为查询的上下文，使用提示模板。在这3个函数之后，只需将所有代码替换为以下内容：

```python
# Load our local index vector db
index = FAISS.load_local("my_faiss_index", embeddings)

# create the prompt template
template = """
Please use the following context to answer questions.
Context: {context}
---
Question: {question}
Answer: Let's think step by step."""

# Hardcoded question
question = "What is a PLC and what is the difference with a PC"
matched_docs, sources = similarity_search(question, index)
# Creating the context
context = "\n".join([doc.page_content for doc in matched_docs])
# instantiating the prompt template and the GPT4All chain
prompt = PromptTemplate(template=template, input_variables=["context", "question"]).partial(context=context)
llm_chain = LLMChain(prompt=prompt, llm=llm)
# Print the result
print(llm_chain.run(question))
```

运行之后你会得到一个这样的结果：

```vbnet
Please use the following context to answer questions.
Context: 1.What is a PLC
2.Where and Why it is used
3.How a PLC is different from a PC
PLC is especially important in industries where safety and reliability are
critical, such as manufacturing plants, chemical plants, and power plants.
How a PLC is different from a PC
Because a PLC is a specialized computer used in industrial and
manufacturing applications to control machinery and processes.,the
hardware components of a typical PLC must be able to interact with
industrial device. So a typical PLC hardware include:
---
Question: What is a PLC and what is the difference with a PC
Answer: Let's think step by step. 1) A Programmable Logic Controller (PLC), 
also called Industrial Control System or ICS, refers to an industrial computer 
that controls various automated processes such as manufacturing 
machines/assembly lines etcetera through sensors and actuators connected 
with it via inputs & outputs. It is a form of digital computers which has 
the ability for multiple instruction execution (MIE), built-in memory 
registers used by software routines, Input Output interface cards(IOC) 
to communicate with other devices electronically/digitally over networks 
or buses etcetera
2). A Programmable Logic Controller is widely utilized in industrial 
automation as it has the ability for more than one instruction execution. 
It can perform tasks automatically and programmed instructions, which allows 
it to carry out complex operations that are beyond a 
Personal Computer (PC) capacity. So an ICS/PLC contains built-in memory 
registers used by software routines or firmware codes etcetera but 
PC doesn't contain them so they need external interfaces such as 
hard disks drives(HDD), USB ports, serial and parallel 
communication protocols to store data for further analysis or 
report generation.
```

如果你希望使用用户输入来替换问题：

```python
question = "What is a PLC and what is the difference with a PC"
```

就像下面这样：

```python
question = input("Your question: ")
```

 

# 结论

现在是你进行实验的时候了。对与你的文档相关的所有主题提出不同的问题，并观察结果。在 Prompt 和模板方面，肯定还有很大的改进空间：你可以在这里找到一些灵感。而且 Langchain 的文档真的很棒（我都能够理解！）。

## MiniGPT-4

近日，来自阿卜杜拉国王科技大学的研究团队，便提出了一个具有类似 GPT-4 图像理解与对话能力的 AI 大模型——MiniGPT-4，并将其开源![[哇]](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a18f9afe2c7e4788a8ea8ffd22ac51a0~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)。

据介绍，MiniGPT-4 具有出色的多模态能力，如从手写草稿创建网站、生成详细的图像描述、根据图像创作故事和诗歌、为图像中描述的问题提供解决方案，以及根据食物照片教对话对象如何烹饪一道美味的菜品等![[让我看看]](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a18f9afe2c7e4788a8ea8ffd22ac51a0~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)。

在技术层面上，MiniGPT-4 由一个带有预训练的 ViT 和 Q-Former 的视觉编码器、一个单一的线性投影层和一个 Vicuna 大语言模型组成。而且，MiniGPT-4 只需要训练线性层，使视觉特征与 Vicuna 保持一致。

有 Y Combinator 用户这样评价 MiniGPT-4，“在技术层面上，他们正在做一些非常简单的事情......但结果非常惊人。最重要的是，它在 OpenAI 的 GPT-4 图像模态之前出现。（这是）开源 AI 的真正胜利。”

也有用户表示，“我认为他们为一个不相关的项目使用 GPT-4 名称是一种糟糕的形式。毕竟，底层的 Vicuna 只是一个微调的 LLaMA。另外，他们使用了较小的 13B 版本。然而，结果看起来很有趣。”

图文生成

本地电脑直接部署运行：要求显存12G

开源地址：<https://github.com/Vision-CAIR/MiniGPT-4>

项目地址：
[minigpt-4.github.io/](https://minigpt-4.github.io/)

## 闻达-Wenda 大语言模型调用平台，搭建专属知识库

【【AI知识助理】再也不用读书啦！让收藏夹的文章都秒变知识！用langChain+ChatGLM-6B清华开源模型，附详细安装教程+下载链接】<https://www.bilibili.com/video/BV1NW4y1R7aQ?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

1.闻达Github主页地址：<https://github.com/wenda-LLM/wenda>

2.懒人一键包地址：<https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz>    提取码：lyqz

## MetaGPT

开源地址：[geekan/MetaGPT：多代理元编程框架：给定一行需求，返回 PRD、设计、任务、存储库 (github.com)](https://github.com/geekan/MetaGPT)

## 开源框架LangChain：允许与AI一起工作的开发人员将大型语言模型

- 开源地址：[hwchase17/langchain: ⚡ Building applications with LLMs through composability ⚡ (github.com)](https://github.com/hwchase17/langchain)
- js版开源地址：[hwchase17/langchainjs (github.com)](https://github.com/hwchase17/langchainjs)
- 文档：[Introduction | 🦜️🔗 Langchain](https://python.langchain.com/docs/get_started/introduction.html)

LangChain是一个开源的框架，允许与AI一起工作的开发人员将大型语言模型，如GPT-4，与外部的计算和数据源结合起来。

教程：【AI必学知识点！Langchain到底是什么？开源干货详细解析！赚钱机会和未来展望！】<https://www.bilibili.com/video/BV1GL411e7K4?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

【5分钟学会搭建本地知识库聊天机器人(基于GPT4+Llamaindex+LangChain)】<https://www.bilibili.com/video/BV18o4y1N7Dm?vd_source=36c9491a7fa2ab8a22ca060af01b7472>

## 『LangChain 中文入门教程』GitHub 1.3K Star，内容巨完整！

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d2b4477066554e4eb909981262629f9d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

LangChain 是一个用于开发由语言模型驱动的应用程序的框架，主要拥有「**将 LLM 模型与外部数据源进行连接**」「**允许与 LLM 模型进行交互**」这2个能力，已经在GitHub获得35K Star，并且快速迭代中。

这篇文章结合官方文档，对 LangChain 的基础知识和实战案进行了全面的整理，并附上了详细的参考资料链接与所有案例代码，希望帮助读者结合 LangChain 开发出更有创意的产品。

**介绍**

> *▢* LangChain
>
> *▢* Doc

**基础功能**

> *▢* LLM 调用
>
> *▢* Prompt管理，支持各种自定义模板
>
> *▢* 拥有大量的文档加载器，比如 Email、Markdown、PDF、Youtube ...
>
> *▢* 对索引的支持
>
> *▢* Chains

**必知概念**

> *▢* Loader 加载器
>
> *▢* Document 文档
>
> *▢* Text Spltters 文本分割
>
> *▢* Vectorstores 向量数据库
>
> *▢* Chain 链
>
> *▢* Agent 代理
>
> *▢* Embedding

**实战**

> *▢* 完成一次问答
>
> *▢* 通过 Google 搜索并返回答案
>
> *▢* 对超长文本进行总结
>
> *▢* 构建本地知识库问答机器人
>
> *▢* 构建向量索引数据库
>
> *▢* 使用 GPT3.5 模型构建油管频道问答机器人
>
> *▢* 用OpenAl连接万种工具
>
> *▢* 一些有意思的小 Tip

**总结**

> *▢* 范例代码  [**⋙ GitHub**](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide) | [**GitBook**](https://liaokong.gitbook.io/llm-kai-fa-jiao-cheng/) | [**范例代码**](https://colab.research.google.com/drive/1ArRVMiS-YkhUlobHrU6BeS8fF57UeaPQ%3Fusp%3Dsharing)

## CahtGPT客户端开源项目合集

### chatbox：ChatGPT开源客户端，提示词调试神器

开源的 ChatGPT 桌面客户端，提示词调试与管理工具，支持 Windows、Mac 和 Linux；内置了一些提示词角色，使用体验比较不错

- 项目地址：<https://github.com/Bin-Huang/chatbox>
- web版地址：<https://web.chatboxapp.xyz/>
- 客户端下载地址：<https://github.com/Bin-Huang/chatbox/releases>

【chatbox：ChatGPT开源客户端，提示词调试神器】<https://www.bilibili.com/video/BV1Am4y1t7X4?vd_source=36c9491a7fa2ab8a22ca060af01b7472>
